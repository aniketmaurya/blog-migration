{
  
    
        "post0": {
            "title": "TorchData: PyTorch Data loading utility library",
            "content": "In this tutorial, we will learn about TorchData . . !pip install torchdata -q . import os.path import re import torch from torch.utils.data.datapipes.utils.decoder import imagehandler, mathandler from torchdata.datapipes.iter import ( FileOpener, Filter, IterableWrapper, IterKeyZipper, Mapper, RoutedDecoder, TarArchiveLoader,FileLister,CSVParser, Filter, ) from PIL import Image from torch.utils.data import DataLoader from torchvision.transforms.functional import to_tensor . ROOT = &quot;/Users/aniket/datasets/cifar-10/train&quot; . csv_dp = FileLister(f&quot;{ROOT}/../trainLabels.csv&quot;) csv_dp = FileOpener(csv_dp) csv_dp = csv_dp.parse_csv() csv_dp = Filter(csv_dp, lambda x: x[1]!=&quot;label&quot;) labels = {e: i for i, e in enumerate(set([e[1] for e in csv_dp]))} . /Users/aniket/miniconda3/envs/am/lib/python3.9/site-packages/torch/utils/data/datapipes/iter/selecting.py:54: UserWarning: Lambda function is not supported for pickle, please use regular python function or functools.partial instead. warnings.warn(&#34;Lambda function is not supported for pickle, please use &#34; . x = iter(csv_dp) next(x) . [&#39;1&#39;, &#39;frog&#39;] . def get_filename(data): idx, label = data return f&quot;{ROOT}/{idx}.png&quot;, label def load_image(data): file, label = data return Image.open(file), label def process(data): img, label = data return to_tensor(img), labels[label] . dp = csv_dp.map(get_filename) dp = dp.map(load_image) dp = dp.map(process) . dl = DataLoader( dp, batch_size=4, shuffle=True, ) . next(iter(dl))[0].shape . torch.Size([4, 3, 32, 32]) .",
            "url": "https://aniketmaurya.com/pytorch/2022/04/09/TorchData.html",
            "relUrl": "/pytorch/2022/04/09/TorchData.html",
            "date": " ‚Ä¢ Apr 9, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "HappyWhale üê≥:PyTorch Training from scratch Lite ‚ö°Ô∏è",
            "content": "In this Notebook article, you will learn how to write a custom training loop in pure PyTorch, create custom torch Dataset class, compute metrics for model performance, and Scale the Training on any hardware like GPU, TPU, IPU or Distributed Training with LightningLite. . Checkout the original Kaggle Notebook here. . . &#128373; Explore the provided data . (EDA is taken from Notebook of Jirka) . !ls -l /kaggle/input/happy-whale-and-dolphin PATH_DATASET = &quot;/kaggle/input/happy-whale-and-dolphin&quot; . total 4668 -rw-r--r-- 1 nobody nogroup 2404234 Feb 1 16:45 sample_submission.csv drwxr-xr-x 2 nobody nogroup 0 Feb 1 16:47 test_images -rw-r--r-- 1 nobody nogroup 2371769 Feb 1 16:47 train.csv drwxr-xr-x 2 nobody nogroup 0 Feb 1 16:51 train_images . Browsing the metadata . import os import pandas as pd import seaborn as sn import matplotlib.pyplot as plt sn.set() df_train = pd.read_csv(os.path.join(PATH_DATASET, &quot;train.csv&quot;)) display(df_train.head()) print(f&quot;Dataset size: {len(df_train)}&quot;) print(f&quot;Unique ids: {len(df_train[&#39;individual_id&#39;].unique())}&quot;) . image species individual_id . 0 00021adfb725ed.jpg | melon_headed_whale | cadddb1636b9 | . 1 000562241d384d.jpg | humpback_whale | 1a71fbb72250 | . 2 0007c33415ce37.jpg | false_killer_whale | 60008f293a2b | . 3 0007d9bca26a99.jpg | bottlenose_dolphin | 4b00fe572063 | . 4 00087baf5cef7a.jpg | humpback_whale | 8e5253662392 | . Dataset size: 51033 Unique ids: 15587 . Lets see how many speaced we have in the database... . counts_imgs = df_train[&quot;species&quot;].value_counts() counts_inds = df_train.drop_duplicates(&quot;individual_id&quot;)[&quot;species&quot;].value_counts() ax = pd.concat({&quot;per Images&quot;: counts_imgs, &quot;per Individuals&quot;: counts_inds}, axis=1).plot.barh(grid=True, figsize=(7, 10)) ax.set_xscale(&#39;log&#39;) . And compare they with unique individuals... . Note: that the counts are in log scale . import numpy as np from pprint import pprint species_individuals = {} for name, dfg in df_train.groupby(&quot;species&quot;): species_individuals[name] = dfg[&quot;individual_id&quot;].value_counts() si_max = max(list(map(len, species_individuals.values()))) si = {n: [0] * si_max for n in species_individuals} for n, counts in species_individuals.items(): si[n][:len(counts)] = list(np.log(counts)) si = pd.DataFrame(si) . import seaborn as sn fig = plt.figure(figsize=(10, 8)) ax = sn.heatmap(si[:500].T, cmap=&quot;BuGn&quot;, ax=fig.gca()) . And see the top individulas . ax = df_train[&quot;individual_id&quot;].value_counts(ascending=True)[-50:].plot.barh(figsize=(3, 8), grid=True) # ascending=True . Browse some images . nb_species = len(df_train[&quot;species&quot;].unique()) fig, axarr = plt.subplots(ncols=5, nrows=nb_species, figsize=(12, nb_species * 2)) for i, (name, dfg) in enumerate(df_train.groupby(&quot;species&quot;)): axarr[i, 0].set_title(name) for j, (_, row) in enumerate(dfg[:5].iterrows()): im_path = os.path.join(PATH_DATASET, &quot;train_images&quot;, row[&quot;image&quot;]) img = plt.imread(im_path) axarr[i, j].imshow(img) axarr[i, j].set_axis_off() . # !pip install -q -U timm pytorch-lightning&gt;=1.6 . import torch import timm from PIL import Image from torchvision import transforms as T from torch.utils.data import Dataset, DataLoader, random_split from torchmetrics import F1 import torch.nn.functional as F from tqdm.auto import tqdm import torch from torch.optim.lr_scheduler import StepLR . Create DataLoader &#128452;&#65039; . DataLoader is an iterable object which contains your input image data and the target label. To create a DataLoader, we first need to implement a torch Dataset class. We define MyDataset class which inherits from Dataset and it will implement __len__ and __getitem__ method. . label_to_idx = {e:i for i, e in enumerate(df_train.species.unique())} class MyDataset(Dataset): def __init__(self, df, transforms=None): super().__init__() self.df = df self.root = os.path.join(PATH_DATASET, &quot;train_images&quot;) self.transforms = transforms def __len__(self): return len(self.df) def __getitem__(self, idx): data = self.df.iloc[idx] image = Image.open(self.root + f&quot;/{data.image}&quot;).convert(&quot;RGB&quot;) label = label_to_idx[data.species] if self.transforms: image = self.transforms(image) return image, label def random_split_dataset(data: Dataset, pct=0.9): &quot;&quot;&quot; Randomly splits dataset into two sets. Length of first split is len(data) * pct. Source: https://github.com/gradsflow/gradsflow/blob/main/gradsflow/data/common.py#L20 Args: data: pytorch Dataset object with `__len__` implementation. pct: percentage of split. &quot;&quot;&quot; n = len(data) split_1 = int(n * pct) split_2 = n - split_1 return random_split(data, (split_1, split_2)) . We define image augmentation to make our classifier robust. We will use the function random_split_dataset to split dataset into train and validation set. Once we have our Dataset object, we can create a DataLoader class like this - dataloader = DataLoader(dataset, batch_size=8) . transforms = T.Compose([T.AutoAugment(), T.Resize((224,224)), T.ToTensor()]) ds = MyDataset(df_train, transforms) split_pct = 0.9 train_ds, val_ds = random_split_dataset(ds, 0.9) . Write Training Loop &#128736;&#65039; . Now, we have our dataloader ready we can create our classifier and write training loop. A training loop consists of model prediction, loss computation, backward propagation and model weight update by the optimizer. . We will start with a basic training loop then will use LightningLite to enable multiple hardware, precision and distributed training. . First we will create model, optimizer, loss function and metrics. . batch_size = 4 device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot; dry_run = True num_epochs = 5 model = timm.create_model(&quot;efficientnet_b0&quot;, pretrained=True, num_classes=len(label_to_idx)) model = model.to(device) optimizer = torch.optim.AdamW(model.parameters(), 1e-4) criterion = torch.nn.CrossEntropyLoss() metric = F1().to(device) train_loader = DataLoader(train_ds, batch_size=batch_size) val_loader = DataLoader(val_ds, batch_size=batch_size) . Downloading: &#34;https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b0_ra-3dd342df.pth&#34; to /root/.cache/torch/hub/checkpoints/efficientnet_b0_ra-3dd342df.pth . Training Loop . For writing the training loop we will iterate a for loop for given number of epochs num_epochs. Set the model to training mode with model.train() and iterate through the dataloader. We pass the data to model and calculate the crossentropy loss. We do loss.backward() to compute gradients followed by optimizer.step() which will update the model weights. . For model evaluation we define a validation loop which will calculate the F1 accuracy on the validation dataset. For validation we set our model to eval mode with model.eval() method. For calculating F1 accuracy, we use TorchMetrics which contains a collection of Machine Learning metrics for distributed, scalable PyTorch models and an easy-to-use API to create custom metrics. . # EPOCH LOOP for epoch in tqdm(range(1, num_epochs + 1)): # TRAINING LOOP model.train() for batch_idx, (data, target) in tqdm(enumerate(train_loader), total=len(train_ds)//batch_size): data, target= data.to(device), target.to(device) optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() if (batch_idx == 0) or ((batch_idx + 1) % log_interval == 0): print( &quot;Train Epoch: {} [{}/{} ({:.0f}%)] tLoss: {:.6f}&quot;.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100.0 * batch_idx / len(train_loader), loss.item(), ) ) if dry_run: break # TESTING LOOP model.eval() test_loss = 0 with torch.no_grad(): for data, target in val_loader: data = data.to(device) target = target.to(device) output = model(data) test_loss += criterion(output, target).item() # WITH TorchMetrics metric(output, target) if dry_run: break # all_gather is used to aggregated the value across processes test_loss = test_loss / len(val_loader.dataset) print(f&quot; nTest set: Average loss: {test_loss:.4f}, Accuracy: ({metric.compute():.0f}%) n&quot;) metric.reset() if dry_run: break . Train Epoch: 1 [0/45929 (0%)] Loss: 3.646841 Test set: Average loss: 0.0008, Accuracy: (0%) . &#128119; Scale Model Training . Our dry run was successful üéâ! Now, let&#39;s scale our training on a hardware accelerator like GPU or TPU. We can also use distributed training if multiple devices are available. For this purpose we use LightningLite, it scales PyTorch model training loop with minimal changes. That means we will retain the full control of our training loop! It also enables Precision support abd DDP training. . To use LightningLite, we will import it from PyTorch Lightning library. We implement LightningLite and override run method. We can just copy paste our whole training loop code inside the run method and then just make these three changes to our code. . model, optimizer = self.setup(model, optimizer) | dataloader = self.setup_dataloaders(dataloader) | Replace loss.backward() with self.backward(loss) | . from pytorch_lightning.lite import LightningLite class CustomTrainer(LightningLite): def run(self, num_epochs, batch_size, gamma=0.7, dry_run: bool=False, save_model=True, log_interval=10): model = timm.create_model(&quot;efficientnet_b0&quot;, pretrained=True, num_classes=len(label_to_idx)) optimizer = torch.optim.AdamW(model.parameters(), 1e-4) criterion = torch.nn.CrossEntropyLoss() metric = F1().to(self.device) print(self.device) # don&#39;t forget to call `setup` to prepare for model / optimizer for distributed training. # the model is moved automatically to the right device. model, optimizer = self.setup(model, optimizer) pin_memory = &quot;cuda&quot; in self.device.type train_loader, val_loader = self.setup_dataloaders(DataLoader(train_ds, batch_size=batch_size, pin_memory=pin_memory), DataLoader(val_ds, batch_size=batch_size, pin_memory=pin_memory)) scheduler = StepLR(optimizer, step_size=1, gamma=gamma) # EPOCH LOOP for epoch in tqdm(range(1, num_epochs + 1)): # TRAINING LOOP model.train() for batch_idx, (data, target) in tqdm(enumerate(train_loader), total=len(train_ds)//batch_size): # NOTE: no need to call `.to(device)` on the data, target optimizer.zero_grad() output = model(data) loss = criterion(output, target) self.backward(loss) # instead of loss.backward() optimizer.step() if (batch_idx == 0) or ((batch_idx + 1) % log_interval == 0): print( &quot;Train Epoch: {} [{}/{} ({:.0f}%)] tLoss: {:.6f}&quot;.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100.0 * batch_idx / len(train_loader), loss.item(), ) ) if dry_run: break scheduler.step() # TESTING LOOP model.eval() test_loss = 0 with torch.no_grad(): for data, target in val_loader: # NOTE: no need to call `.to(device)` on the data, target output = model(data) test_loss += criterion(output, target).item() # WITH TorchMetrics metric(output, target) if dry_run: break # all_gather is used to aggregated the value across processes test_loss = self.all_gather(test_loss).sum() / len(val_loader.dataset) print(f&quot; nTest set: Average loss: {test_loss:.4f}, Accuracy: ({metric.compute():.0f}%) n&quot;) metric.reset() if dry_run: break # When using distributed training, use `self.save` # to ensure the current process is allowed to save a checkpoint if save_model: self.save(model.state_dict(), &quot;model.pt&quot;) . That&#39;s all we need to do. Now we can select any supported hardware, precision type, number of devices, or training strategy. . Run this cell to train the model on one GPU. . trainer = CustomTrainer(accelerator = &quot;gpu&quot;, gpus=1) trainer.run(num_epochs=1, batch_size=128) . cuda:0 Train Epoch: 1 [0/45929 (0%)] Loss: 4.033646 Train Epoch: 1 [1152/45929 (3%)] Loss: 2.897996 Train Epoch: 1 [2432/45929 (5%)] Loss: 2.228965 Train Epoch: 1 [3712/45929 (8%)] Loss: 1.733439 Train Epoch: 1 [4992/45929 (11%)] Loss: 1.733582 Train Epoch: 1 [6272/45929 (14%)] Loss: 1.438736 Train Epoch: 1 [7552/45929 (16%)] Loss: 1.324237 Train Epoch: 1 [8832/45929 (19%)] Loss: 1.311875 Train Epoch: 1 [10112/45929 (22%)] Loss: 1.153679 Train Epoch: 1 [11392/45929 (25%)] Loss: 1.068135 Train Epoch: 1 [12672/45929 (28%)] Loss: 0.795189 Train Epoch: 1 [13952/45929 (30%)] Loss: 0.806610 Train Epoch: 1 [15232/45929 (33%)] Loss: 0.847775 Train Epoch: 1 [16512/45929 (36%)] Loss: 0.828618 Train Epoch: 1 [17792/45929 (39%)] Loss: 0.805902 Train Epoch: 1 [19072/45929 (42%)] Loss: 0.794924 Train Epoch: 1 [20352/45929 (44%)] Loss: 0.730848 Train Epoch: 1 [21632/45929 (47%)] Loss: 0.791776 Train Epoch: 1 [22912/45929 (50%)] Loss: 0.567644 Train Epoch: 1 [24192/45929 (53%)] Loss: 0.712434 Train Epoch: 1 [25472/45929 (55%)] Loss: 0.621673 Train Epoch: 1 [26752/45929 (58%)] Loss: 0.657078 Train Epoch: 1 [28032/45929 (61%)] Loss: 0.829939 Train Epoch: 1 [29312/45929 (64%)] Loss: 0.632966 Train Epoch: 1 [30592/45929 (67%)] Loss: 0.603066 Train Epoch: 1 [31872/45929 (69%)] Loss: 0.510153 Train Epoch: 1 [33152/45929 (72%)] Loss: 0.507989 Train Epoch: 1 [34432/45929 (75%)] Loss: 0.515971 Train Epoch: 1 [35712/45929 (78%)] Loss: 0.410823 Train Epoch: 1 [36992/45929 (81%)] Loss: 0.316385 Train Epoch: 1 [38272/45929 (83%)] Loss: 0.455764 Train Epoch: 1 [39552/45929 (86%)] Loss: 0.532778 Train Epoch: 1 [40832/45929 (89%)] Loss: 0.457156 Train Epoch: 1 [42112/45929 (92%)] Loss: 0.447986 Train Epoch: 1 [43392/45929 (94%)] Loss: 0.355336 Train Epoch: 1 [44672/45929 (97%)] Loss: 0.368111 Test set: Average loss: 0.0030, Accuracy: (1%) . Happy Training! &#9889;&#65039;&#127881; .",
            "url": "https://aniketmaurya.com/kaggle/2022/04/03/happywhale-pytorch-training-from-scratch-lite.html",
            "relUrl": "/kaggle/2022/04/03/happywhale-pytorch-training-from-scratch-lite.html",
            "date": " ‚Ä¢ Apr 3, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Deploying PyTorch in Production with TorchServe",
            "content": "We will discuss about Model Archiever, writing custom Handler and torchserve inferene APIs. . Handler . In plain and simple words, Handler is a Python module that defines few methods that initializes the model, preprocess data, infer from model and post-process the output (Init-Pre-Infer-Post). . Initialize the model instance | Pre-process input data before it is sent to the model for inference | Customize how the model is invoked for inference or explanations | Post-process output from the model before sending the response to the user | BaseHandler is an Abstract class for Handler which already implements most of the functionality. . Methods of BaseHandler class: initialize, preprocess, postprocess, handle, explain_handle We may need to override preprocess or postprocess based on our use case! . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "https://aniketmaurya.com/pytorch/torchserve/mlops/production/2021/03/09/Deploying-PyTorch-in-Production-with-TorchServe.html",
            "relUrl": "/pytorch/torchserve/mlops/production/2021/03/09/Deploying-PyTorch-in-Production-with-TorchServe.html",
            "date": " ‚Ä¢ Mar 9, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Pix2Pix - Image to image translation with Conditional Adversarial Networks",
            "content": "Author Introduction . Hi! My name is Aniket Maurya. I am a Machine Learning Engineer at Quinbay Technologies, India. I research and build ML products for an e-commerce giant. I like to share my limited knowledge of Machine Learning and Deep Learning with on my blog or YouTube channel. You can connect with me on Linkedin/Twitter. . Introduction to Conditional Adversarial Networks . Image to Image translation means transforming the given source image into a different image. Gray scale image to colour image conversion is one such example of image of image translation. . In this tutorial we will discuss GANs, a few points from Pix2Pix paper and implement the Pix2Pix network to translate segmented facade into real pictures. We will create the Pix2Pix model in PyTorch and use PyTorch lightning to avoid boilerplates. . GANs are Generative models that learns a mapping from random noise vector to an output image. G(z) -&gt; Image (y) . For example, GANs can learn mapping from random normal vectors to generate smiley images. For training such a GAN we just need a set of smiley images and train the GAN with an adversarial loss üôÇ. After the model is trained we can use random normal noise vectors to generate images that were not in the training dataset. . But what if we want to build a network in such a way that we can control what the model will generate. In our case we want the model to generate a laughing smiley. . Conditional GANs are Generative networks which learn mapping from random noise vectors and a conditional vector to output an image. Suppose we have 4 types of smileys - smile, laugh, sad and angry (üôÇ üòÇ üòî üò°). So our class vector for smile üôÇ can be (1,0,0,0), laugh can be üòÇ (0,1,0,0) and similarly for others. Here the conditional vector is the smiley embedding. . During training of the generator the conditional image is passed to the generator and fake image is generated. The fake image is then passed through the discriminator along with the conditional image, both fake image and conditional image are concatenated. Discriminator penalizes the generator if it correctly classifies the fake image as fake. . Pix2Pix . Pix2Pix is an image-to-image translation Generative Adversarial Networks that learns a mapping from an image X and a random noise Z to output image Y or in simple language it learns to translate the source image into a different image. . During the time Pix2Pix was released, several other works were also using Conditional GANs on discrete labels. Pix2Pix uses a U-Net based architecture for the Generator and for the Discriminator a PathGAN Classifier is used. . . Pix2Pix Generator is an U-Net based architecture which is an encoder-decoder network with skip connections. Both generator and discriminator uses Convolution-BatchNorm-ReLu like module or in simple words we can say that it is the unit block of the generator and discriminator. Skip connections are added between each layer i and layer n ‚àí i, where n is the total number of layers. At each skip connection all the channels from current layer i are concatenated with all the channels at n-i layer. . Lets understand more with code . # collapse-hide import os from glob import glob from pathlib import Path import matplotlib.pyplot as plt import pytorch_lightning as pl import torch from PIL import Image from torch import nn from torch.utils.data import DataLoader, Dataset from torchvision import transforms from torchvision.transforms.functional import center_crop from torchvision.utils import make_grid from tqdm.auto import tqdm . . Uncomment the below code to download the dataset . # !wget http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/facades.tar.gz # !tar -xvf facades.tar.gz # http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/facades.tar.gz . After downloading the dataset we create Dataloader which loads our conditional and real image. . path = &quot;../../pytorch-gans/pix2pix/facades/train/&quot; class FacadesDataset(Dataset): def __init__(self, path, target_size=None): self.filenames = glob(str(Path(path) / &quot;*&quot;)) self.target_size = target_size def __len__(self): return len(self.filenames) def __getitem__(self, idx): filename = self.filenames[idx] image = Image.open(filename) image = transforms.functional.to_tensor(image) image_width = image.shape[2] real = image[:, :, : image_width // 2] condition = image[:, :, image_width // 2 :] target_size = self.target_size if target_size: condition = nn.functional.interpolate(condition, size=target_size) real = nn.functional.interpolate(real, size=target_size) return real, condition . We create the unit module that will be used in Generator and Discriminator (Convolution-&gt;BatchNorm-&gt;ReLu). We also keep our option open to use DropOut layer when we need. . class ConvBlock(nn.Module): &quot;&quot;&quot; Unit block of the Pix2Pix &quot;&quot;&quot; def __init__(self, in_channels, out_channels, use_dropout=False, use_bn=True): super().__init__() self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1) if use_bn: self.batchnorm = nn.BatchNorm2d(out_channels) self.use_bn = use_bn if use_dropout: self.dropout = nn.Dropout() self.use_dropout = use_dropout self.activation = nn.LeakyReLU(0.2) def forward(self, x): x = self.conv1(x) if self.use_bn: x = self.batchnorm(x) if self.use_dropout: x = self.dropout(x) x = self.activation(x) return x . In the first part of U-Net network the layer size decreases, we create a DownSampleConv module for this. This module will contain the unit block that we just created ConvBlock. . class DownSampleConv(nn.Module): def __init__(self, in_channels, use_dropout=False, use_bn=False): super().__init__() self.conv_block1 = ConvBlock(in_channels, in_channels * 2, use_dropout, use_bn) self.conv_block2 = ConvBlock( in_channels * 2, in_channels * 2, use_dropout, use_bn ) self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2) def forward(self, x): x = self.conv_block1(x) x = self.conv_block2(x) x = self.maxpool(x) return x . Now in the second part the network expands and so we create UpSampleConv . class UpSampleConv(nn.Module): def __init__(self, input_channels, use_dropout=False, use_bn=True): super().__init__() self.upsample = nn.Upsample(scale_factor=2, mode=&quot;bilinear&quot;, align_corners=True) self.conv1 = nn.Conv2d(input_channels, input_channels // 2, kernel_size=2) self.conv2 = nn.Conv2d( input_channels, input_channels // 2, kernel_size=3, padding=1 ) self.conv3 = nn.Conv2d( input_channels // 2, input_channels // 2, kernel_size=2, padding=1 ) if use_bn: self.batchnorm = nn.BatchNorm2d(input_channels // 2) self.use_bn = use_bn self.activation = nn.ReLU() if use_dropout: self.dropout = nn.Dropout() self.use_dropout = use_dropout def forward(self, x, skip_con_x): x = self.upsample(x) x = self.conv1(x) skip_con_x = center_crop(skip_con_x, x.shape[-2:]) x = torch.cat([x, skip_con_x], axis=1) x = self.conv2(x) if self.use_bn: x = self.batchnorm(x) if self.use_dropout: x = self.dropout(x) x = self.activation(x) x = self.conv3(x) if self.use_bn: x = self.batchnorm(x) if self.use_dropout: x = self.dropout(x) x = self.activation(x) return x . Now the basic blocks of the Pix2Pix generated is created, we create the generator module. Generator is formed of expanding and contracting layers. The first part network contracts and then expands again, i.e. first we have encoder block and then decoder block. Below is the encoder-decoder of U-Net network configuration from official paper. Here C denotes the unit block that we created ConvBlock and D denotes Drop Out with value 0.5. In the decoder, the output tensors from n-i layer of encoder concatenates with i layer of the decoder. Also the first three blocks of the decoder has drop out layers. . Encoder: C64-C128-C256-C512-C512-C512-C512-C512 Decoder: CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128 . class Generator(nn.Module): def __init__(self, in_channels, out_channels, hidden_channels=32, depth=6): super().__init__() self.conv1 = nn.Conv2d(in_channels, hidden_channels, kernel_size=1) self.conv_final = nn.Conv2d(hidden_channels, out_channels, kernel_size=1) self.depth = depth self.contracting_layers = [] self.expanding_layers = [] self.sigmoid = nn.Sigmoid() # encoding/contracting path of the Generator for i in range(depth): down_sample_conv = DownSampleConv( hidden_channels * 2 ** i, ) self.contracting_layers.append(down_sample_conv) # decoder/Expanding path of the Generator for i in range(depth): upsample_conv = UpSampleConv( hidden_channels * 2 ** (i + 1), use_dropout=(True if i &lt; 3 else False) ) self.expanding_layers.append(upsample_conv) self.contracting_layers = nn.ModuleList(self.contracting_layers) self.expanding_layers = nn.ModuleList(self.expanding_layers) def forward(self, x): depth = self.depth contractive_x = [] x = self.conv1(x) contractive_x.append(x) for i in range(depth): x = self.contracting_layers[i](x) contractive_x.append(x) for i in range(depth - 1, -1, -1): x = self.expanding_layers[i](x, contractive_x[i]) x = self.conv_final(x) return self.sigmoid(x) . Discriminator . A discriminator is a ConvNet which learns to classify images into discrete labels. In GANs, discriminators learns to predict whether the given image is real or fake. PatchGAN is the discriminator used for Pix2Pix. Its architecture is different from a typical image classification ConvNet because of the output layer size. In convnets output layer size is equal to the number of classes while in PatchGAN output layer size is a 2D matrix. . Now we create our Discriminator - PatchGAN. In this network we use the same DownSampleConv module that we created for generator. . class PatchGAN(nn.Module): def __init__(self, input_channels, hidden_channels=8): super().__init__() self.conv1 = nn.Conv2d(input_channels, hidden_channels, kernel_size=1) self.contract1 = DownSampleConv(hidden_channels, use_bn=False) self.contract2 = DownSampleConv(hidden_channels * 2) self.contract3 = DownSampleConv(hidden_channels * 4) self.contract4 = DownSampleConv(hidden_channels * 8) self.final = nn.Conv2d(hidden_channels * 16, 1, kernel_size=1) def forward(self, x, y): x = torch.cat([x, y], axis=1) x0 = self.conv1(x) x1 = self.contract1(x0) x2 = self.contract2(x1) x3 = self.contract3(x2) x4 = self.contract4(x3) xn = self.final(x4) return xn . Loss Function . Loss function used in Pix2Pix are Adversarial loss and Reconstruction loss. Adversarial loss is used to penalize the generator to predict more realistic images. In conditional GANs, generators job is not only to produce realistic image but also to be near the ground truth output. Reconstruction Loss helps network to produce the realistic image near the conditional image. . adversarial_loss = nn.BCEWithLogitsLoss() reconstruction_loss = nn.L1Loss() . #collapse-hide # https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch def _weights_init(m): if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)): torch.nn.init.normal_(m.weight, 0.0, 0.02) if isinstance(m, nn.BatchNorm2d): torch.nn.init.normal_(m.weight, 0.0, 0.02) torch.nn.init.constant_(m.bias, 0) . . class Pix2Pix(pl.LightningModule): def __init__(self, in_channels, out_channels, hidden_channels=32, depth=6, learning_rate=0.0002, lambda_recon=200): super().__init__() self.save_hyperparameters() self.gen = Generator(in_channels, out_channels, hidden_channels, depth) self.patch_gan = PatchGAN(in_channels + out_channels, hidden_channels=8) # intializing weights self.gen = self.gen.apply(_weights_init) self.patch_gan = self.patch_gan.apply(_weights_init) self.adversarial_criterion = nn.BCEWithLogitsLoss() self.recon_criterion = nn.L1Loss() def _gen_step(self, real_images, conditioned_images): # Pix2Pix has adversarial and a reconstruction loss # First calculate the adversarial loss fake_images = self.gen(conditioned_images) disc_logits = self.patch_gan(fake_images, conditioned_images) adversarial_loss = self.adversarial_criterion(disc_logits, torch.ones_like(disc_logits)) # calculate reconstruction loss recon_loss = self.recon_criterion(fake_images, real_images) lambda_recon = self.hparams.lambda_recon return adversarial_loss + lambda_recon * recon_loss def _disc_step(self, real_images, conditioned_images): fake_images = self.gen(conditioned_images).detach() fake_logits = self.patch_gan(fake_images, conditioned_images) real_logits = self.patch_gan(real_images, conditioned_images) fake_loss = self.adversarial_criterion(fake_logits, torch.zeros_like(fake_logits)) real_loss = self.adversarial_criterion(real_logits, torch.ones_like(real_logits)) return (real_loss + fake_loss) / 2 def configure_optimizers(self): lr = self.hparams.learning_rate gen_opt = torch.optim.Adam(self.gen.parameters(), lr=lr) disc_opt = torch.optim.Adam(self.patch_gan.parameters(), lr=lr) return disc_opt, gen_opt def training_step(self, batch, batch_idx, optimizer_idx): real, condition = batch loss = None if optimizer_idx == 0: loss = self._disc_step(real, condition) self.log(&#39;PatchGAN Loss&#39;, loss) elif optimizer_idx == 1: loss = self._gen_step(real, condition) self.log(&#39;Generator Loss&#39;, loss) if self.current_epoch%50==0 and batch_idx==0 and optimizer_idx==1: fake = self.gen(condition).detach() show_tensor_images(condition[0]) show_tensor_images(real[0]) show_tensor_images(fake[0]) return loss . Now that the network is implemented now we are ready to train. You can also modify the dataloader and train on custom dataset. . Hope you liked the article! Happy training üòÉ . dataset = FacadesDataset(path, target_size=target_size) dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True) pix2pix = Pix2Pix(3, 3) trainer = pl.Trainer(max_epochs=100, gpus=1) trainer.fit(pix2pix, dataloader) .",
            "url": "https://aniketmaurya.com/gans/pytorch/2021/02/13/Pix2Pix-explained-with-code.html",
            "relUrl": "/gans/pytorch/2021/02/13/Pix2Pix-explained-with-code.html",
            "date": " ‚Ä¢ Feb 13, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "DCGAN Tutorial‚Ää-‚ÄäGenerate Fake Celebrity image",
            "content": ". This article can be opened as Jupyter Notebook to train DCGAN on CelebA dataset to generate fake celebrity images. . youtube:https://github.com/aniketmaurya/pytorch-gans . What is DCGAN? . DCGAN (Deep Convolutional Generative Adversarial Network) is created by Alec Radford, Luke Metz and Soumith Chintala in 2016 to train Deep Generative Adversarial Networks. In the DCGAN paper, the authors trained the network to produce fake faces of celebrities and fake bedroom images. . The architecture consists of two networks - Generator and Discriminator. Generator is the heart of GANs. It produces real looking fake images from random noise. . Discriminator wants the real and fake image distributions to be as far as possible while the Generator wants to reduce the distance between the real and fake image distribution. In simple words, the Generator tries to fool the Discriminator by producing real looking images while the Discriminator tries to catch the fake images from the real ones. . . Vector arithmetic for visual concepts. Source: Paper | . Training details from the paper . Preprocessing: Images are scaled to be in range of tanh activation, [-1, 1]. Training was done with a mini-batch size of 128 and Adam optimizer with a learning rate of 0.0002. All the weights initialised with Normal distribution $ mu(0, 0.02)$. . Authors guidelines: . All the pooling layers are replaced with strided convolutions in the discriminator and fractional strided convolution in the discriminator. | No fully-connected or pooling layers are used. | Batchnorm used in both Generator and Discriminator | ReLu activation is used for generator for all the layers except the last layer which uses tanh | Discriminator uses LeakyReLu for all the layers | . In this post I will train a GAN to generate celebrity faces. . Generator . A Generator consists Transposed Convolution, Batch Normalisation and activation function layer. . First the random noise of size 100 will be reshaped to 100x1x1 (channel first in PyTorch). | It is passed through a Transposed CNN layer which upsamples the input Tensor. | Batch Normalisation is applied. | If the layer is not the last layer then ReLu activation is applied else Tanh. | . First channel size is 1024 which is then decreased block by block to 3 for RGB image. Finally we will get a 3x64x64 Tensor which will be our image. . . Generator architecture from the Paper | . #collapse-hide import torch from torch import nn from torch.utils.data import DataLoader from torchvision import datasets, transforms from torchvision.utils import make_grid from torch.optim import Adam from tqdm.notebook import tqdm import matplotlib.pyplot as plt torch.manual_seed(0) . . &lt;torch._C.Generator at 0x7f7b8cf9e6c0&gt; . #collapse-hide def show_tensor_images(image_tensor, num_images=25, size=(3, 64, 64)): &#39;&#39;&#39; Function for visualizing images: Given a tensor of images, number of images, and size per image, plots and prints the images in an uniform grid. &#39;&#39;&#39; image_tensor = (image_tensor + 1) / 2 image_unflat = image_tensor.detach().cpu() image_grid = make_grid(image_unflat[:num_images], nrow=5) plt.imshow(image_grid.permute(1, 2, 0).squeeze()) plt.show() . . class Generator(nn.Module): def __init__(self, in_channels=3, z_dim=100): super(Generator, self).__init__() self.gen = nn.Sequential( self.create_upblock(z_dim, 1024, kernel_size=4, stride=1, padding=0), self.create_upblock(1024, 512, kernel_size=4, stride=2, padding=1), self.create_upblock(512, 256, kernel_size=4, stride=2, padding=1), self.create_upblock(256, 128, kernel_size=4, stride=2, padding=1), self.create_upblock(128, 3, kernel_size=4, stride=2, padding=1, final_layer=True), ) def create_upblock(self, in_channels, out_channels, kernel_size=5, stride=2, padding=1, final_layer=False): if final_layer: return nn.Sequential( nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False), nn.BatchNorm2d(out_channels), nn.Tanh() ) return nn.Sequential( nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False), nn.BatchNorm2d(out_channels), nn.ReLU(True) ) def forward(self, noise): &quot;&quot;&quot; noise: random vector of shape=(N, 100, 1, 1) &quot;&quot;&quot; assert len(noise.shape)==4, &#39;random vector of shape=(N, 100, 1, 1)&#39; return self.gen(noise) . Discriminator . The architecture of a Discriminator is same as that of a normal image classification model. It contains Convolution layers, Activation layer and BatchNormalisation. In the DCGAN paper, strides are used instead of pooling to reduce the size of a kernel. Also there is no Fully Connected layer in the network. Leaky ReLU with leak slope 0.2 is used. . The Discriminator wants to predict the fake images as fake and real images as real. On the other hand the Generator wants to fool Discriminator into predicting the fake images produced by the Generator as real. . . Source: deeplearning.ai GANs Specialisation | . class Discriminator(nn.Module): def __init__(self, im_chan=3, hidden_dim=32): super(Discriminator, self).__init__() self.disc = nn.Sequential( self.make_disc_block(im_chan, hidden_dim), self.make_disc_block(hidden_dim, hidden_dim * 2), self.make_disc_block(hidden_dim*2, hidden_dim * 4, stride=1), self.make_disc_block(hidden_dim*4, hidden_dim * 4, stride=2), self.make_disc_block(hidden_dim * 4, 1, final_layer=True), ) def make_disc_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False): if not final_layer: return nn.Sequential( nn.Conv2d(input_channels, output_channels, kernel_size, stride), nn.BatchNorm2d(output_channels), nn.LeakyReLU(0.2) ) else: return nn.Sequential( nn.Conv2d(input_channels, output_channels, kernel_size, stride) ) def forward(self, image): disc_pred = self.disc(image) return disc_pred.view(len(disc_pred), -1) . Define learning rate, z_dim (noise dimension), batch size and other configuration based on the paper. . #collapse-hide # Configurations are from DCGAN paper z_dim = 100 batch_size = 128 lr = 0.0002 beta_1 = 0.5 beta_2 = 0.999 device = &#39;cuda&#39; . . #collapse-hide def weights_init(m): classname = m.__class__.__name__ if classname.find(&#39;Conv&#39;) != -1: nn.init.normal_(m.weight.data, 0.0, 0.02) elif classname.find(&#39;BatchNorm&#39;) != -1: nn.init.normal_(m.weight.data, 1.0, 0.02) nn.init.constant_(m.bias.data, 0) gen = Generator().to(device) disc = Discriminator().to(device) gen_optimizer = Adam(gen.parameters(), lr, betas=(beta_1, beta_2)) disc_optimizer = Adam(disc.parameters(), lr, betas=(beta_1, beta_2)) gen = gen.apply(weights_init) disc = disc.apply(weights_init) . . #collapse-hide # You can tranform the image values to be between -1 and 1 (the range of the tanh activation) transform=transforms.Compose([ transforms.Resize(64), transforms.CenterCrop(64), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ]) dataloader = DataLoader( datasets.CelebA(&#39;.&#39;, download=True, transform=transform), batch_size=batch_size, shuffle=True) . . Training Loop . Binary Crossentropy loss, $J( theta) = -1/m sum[y^i logh[X^i, theta] + (1-y^i)log(1-h[X^i, theta)]$, for training DCGAN. . Discriminator Loss . As the discriminator wants to increase the distance between Generated and Real distribution, we will train it to give high loss when the generated images is classified as real or when real images are classified as fake. . Generator Loss . The BCE loss for Generator will be high when it fails to fool the Discriminator. It will give high loss when the generated image is classified as fake by the discriminator. Note that the Generator never know about real images. . criterion = nn.BCEWithLogitsLoss() display_step = 500 . n_epochs = 50 cur_step = 0 mean_generator_loss = 0 mean_discriminator_loss = 0 for epoch in range(n_epoch): for real, _ in tqdm(dataloader): real = real.to(device) # update the discriminator # create fake images from random noise disc_optimizer.zero_grad() noise = torch.randn(cur_batch_size, z_dim, 1, 1, device=device) fake_images = gen(noise) logits_fake = disc(fake_images.detach()) logits_real = disc(real) disc_loss_fake = criterion(fake_logits, torch.zeros_like(loss_fake)) disc_loss_real = criterion(real_logits, torch.ones_like(logits_real)) disc_loss = (disc_loss_fake + disc_loss_real) / 2 # Keep track of the average discriminator loss mean_discriminator_loss += disc_avg_loss.item() / display_step disc_loss.backward(retain_graph=True) disc_optimizer.step() # Update the generator gen_optimizer.zero_grad() noise = torch.randn(cur_batch_size, z_dim, 1, 1, device=device) fake_images = gen(noise) logits_fake = disc(fake_images) gen_loss = criterion(logits_fake, torch.ones_like(logits_fake)) gen_loss.backward() gen_optimizer.step() # Keep track of the average generator loss mean_generator_loss += gen_loss.item() / display_step ## Visualization code ## if cur_step % display_step == 0 and cur_step &gt; 0: print(f&quot;Step {cur_step}: Generator loss: {mean_generator_loss}, discriminator loss: {mean_discriminator_loss}&quot;) show_tensor_images(fake_images) show_tensor_images(real) mean_generator_loss = 0 mean_discriminator_loss = 0 cur_step += 1 . References . [1.] Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks . [2.] Generative Adversarial Networks (GANs) Specialization . [3.] DCGAN Tutorial - PyTorch Official . I would highly recommend GANs Specialization on Coursera if you want to learn GANs in depth. .",
            "url": "https://aniketmaurya.com/gans/2020/11/16/DCGAN.html",
            "relUrl": "/gans/2020/11/16/DCGAN.html",
            "date": " ‚Ä¢ Nov 16, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "DCGAN Tutorial - Deep Convolutional Generative Adversarial Network",
            "content": "What is DCGAN? . GCGAN is a Deep Convolutional Generative Adversarial Network created by Alec Radford, Luke Metz and Soumith Chintala in 2016 to train Deep Generative Adversarial Networks. In the DCGAN paper, the authors trained the network to produce fake faces of celebrities and fake bedroom images. . The architecture consists of two networks - Generator and Discriminator. Generator is the heart of GANs. It produces real looking fake images from random noise. . . Image from the paper .",
            "url": "https://aniketmaurya.com/gans/2020/11/16/DCGAN-in-PyTorch-Tutorial.html",
            "relUrl": "/gans/2020/11/16/DCGAN-in-PyTorch-Tutorial.html",
            "date": " ‚Ä¢ Nov 16, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "tf.data: Creating data input pipelines",
            "content": "Are you not able to load your NumPy data into memory? Does your model have to wait for data to be loaded after each epoch? Is your Keras DataGenerator slow? . TensorFlow tf.data API allows building complex input pipelines. It easily handles a large amount of data and can read different formats of data while allowing complex data transformations. . Why do we need tf.data? . A training step involves the following steps: . File reading | Fetch or parse data | Data transformation | Using the data to train the model. | source: Tensorflow . If you have a large amount of data and you‚Äôre unable to load it into the memory, you may want to use Generators. But Generators has limited portability and scalability. . After every epoch, you will wait for data to be transformed into a consumable format by the model and during that time your model sits idle, not doing any training. This leads to low CPU and GPU utilization. . One solution to handle this is to prefetch your data in advance and you won‚Äôt have to wait for data to be loaded. . source: Tensorflow . tf.data is a data input pipeline building API than you can use to easily build your data pipeline. Whether you want to read data from local files or even if your data is stored remotely. . Loading data for classification . To train an image classification model, we create a CNN model and feed our data to the model. I want to train a Cats vs Dogs classifier and my data is stored in the following folder structure. . data ‚îî‚îÄ‚îÄ train ‚îú‚îÄ‚îÄ cat -&gt; contains images of cats ‚îî‚îÄ‚îÄ dog -&gt; contains images of dogs* . We first find the path of all the images- . from glob import glob import tensorflow as tf image_path_list = glob(&#39;data/train/*/*.jpg&#39;) data = tf.data.Dataset.list_files(image_path_list) . tf.data.Dataset.list_files converts the list returned by glob method to the Dataset object. Now, we will load the images and their class. . def load_images(path): image = tf.io.read_file(path) image = tf.io.decode_image(image) label = tf.strings.split(path, os.path.sep)[-2] return image, label data = data.map(load_images) . So, the data object now has images and labels. But this is not it, we will have to resize the image, preprocess and apply transformations. . def preprocess(image, label): image = tf.image.resize(image, (IMG_HEIGHT, IMG_WIDTH)) image = tf.image.random_flip_left_right(image) image = tf.image.random_flip_up_down(image) image /= 255. image -= 0.5 return image, label data = data.map(preprocess) . I have created a small library named Chitra, based on tf.data that can be used to skip all these steps. . from chitra import dataloader as dl path = &#39;./data/train&#39; train_dl = dl.Clf() data = train_dl.from_folder(path, target_shape=(224, 244), shuffle = True) # to visualize the data train_dl.show_batch(6, figsize=(6,6)) . You can just specify the path of your data and it will be loaded with the target size. . . You can find my code at https://github.com/aniketmaurya/chitra .",
            "url": "https://aniketmaurya.com/tensorflow/2020/04/08/tf.data-Creating-Data-Input-Pipelines.html",
            "relUrl": "/tensorflow/2020/04/08/tf.data-Creating-Data-Input-Pipelines.html",
            "date": " ‚Ä¢ Apr 8, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Linear Regression from Scratch",
            "content": "Import libraries . import numpy as np import random import matplotlib.pyplot as plt . Data . X = [*range(1, 51)] Y = list(map(lambda x: 2*x + 5, X)) . Univariate Regression . $h( theta) = theta*X + b$ . MSE cost function . $ sum (h(x) - y)^2$ . Gradient Descent . repeat { √ò = √ò - ‚àÜJ(√ò) = √ò - LR*1/m * sum((h(√ò, b) - Y)*X) b = b - ‚àÜJ(b) = b - LR*1/m * sum((h(√ò, b) - Y)) } . def mse(y_true, y_pred): cost = 0 m = len(y_pred) for i in range(m): cost += (y_pred[i] - y_true[i]) ** 2 return cost/(2*m) def der_mse(y_true, y_pred): der_cost = 0 m = len(y_pred) for i in range(m): der_cost += (y_pred[i] - y_true[i]) return der_cost def predict(x): return w*x + b . # Intialization of variables m = len(X) LR = 0.01 w,b =0,0.1 epochs = 10000 # Training total_cost = [] for i in range(epochs): y_pred = [] epoch_cost = [] for num, data in enumerate(zip(X, Y)): x, y = data y_pred = [] y_pred.append(w*x + b) cost = mse(Y[num:num+1], y_pred) epoch_cost.append(cost) der_cost = der_mse(Y[num:num+1], y_pred) w -= LR * (1/m) * der_cost * x b -= LR * (1/m) * der_cost total_cost.append(np.mean(epoch_cost)) if i%500==0: print(f&#39;epoch:{i} t tcost:{cost}&#39;) . epoch:0 cost:0.024546020195931887 epoch:500 cost:0.0035238913511105277 epoch:1000 cost:0.0004771777468473895 epoch:1500 cost:6.461567040474519e-05 epoch:2000 cost:8.749747634800157e-06 epoch:2500 cost:1.1848222450189964e-06 epoch:3000 cost:1.604393419109384e-07 epoch:3500 cost:2.1725438173628743e-08 epoch:4000 cost:2.9418885555175706e-09 epoch:4500 cost:3.983674896607656e-10 epoch:5000 cost:5.3943803161575866e-11 epoch:5500 cost:7.30464704919418e-12 epoch:6000 cost:9.891380608202818e-13 epoch:6500 cost:1.3394131683086816e-13 epoch:7000 cost:1.8137281109430194e-14 epoch:7500 cost:2.4560089530711338e-15 epoch:8000 cost:3.3257381016463754e-16 epoch:8500 cost:4.5034718706313674e-17 epoch:9000 cost:6.09814092196085e-18 epoch:9500 cost:8.25761584212193e-19 . predict(2), predict(9) . (8.999999990490096, 22.999999991911498) . w, b . (2.000000000203057, 4.999999990083981) . plt.plot(total_cost) plt.show() .",
            "url": "https://aniketmaurya.com/machine%20learning/2020/03/27/Linear-Regression-Scratch.html",
            "relUrl": "/machine%20learning/2020/03/27/Linear-Regression-Scratch.html",
            "date": " ‚Ä¢ Mar 27, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . #collapse-output print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "https://aniketmaurya.com/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://aniketmaurya.com/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "EfficientDet: When Object Detection Meets Scalability and Efficiency",
            "content": "EfficientDet, a highly efficient and scalable state of the art object detection model developed by Google Research, Brain Team. It is not just a single model. It has a family of detectors which achieve better accuracy with an order-of-magnitude fewer parameters and FLOPS than previous object detectors. . EfficientDet paper has mentioned its 7 family members. . Comparison of EfficientDet detectors[0‚Äì6] with other SOTA object detection models. . Source: arXiv:1911.09070v1 . Quick Overview of the Paper . EfficientNet is the backbone architecture used in the model. EfficientNet is also written by the same authors at Google. Conventional CNN models arbitrarily scaled network dimensions- width, depth and resolution. EfficientNet uniformly scales each dimension with a fixed set of scaling coefficients. It surpassed SOTA accuracy with 10x efficiency. . | BiFPN: While fusing (applying residual or skip connections) different input features, most of the works simply summed them up without any distinction. Since both input features are at the different resolutions they don‚Äôt equally contribute to the fused output layer. The paper proposes a weighted bi-directional feature pyramid network (BiFPN), which introduces learnable weights to learn the importance of different input features. . | Compound Scaling: For higher accuracy previous object detection models relied on ‚Äî bigger backbone or larger input image sizes. Compound Scaling is a method that uses a simple compound coefficient œÜ to jointly scale-up all dimensions of the backbone network, BiFPN network, class/box network, and resolution. . | Combining EfficientNet backbones with our propose BiFPN and compound scaling, we have developed a new family of object detectors, named EfficientDet, which consistently achieve better accuracy with an order-of-magnitude fewer parameters and FLOPS than previous object detectors. . BiFPN . Source: arXiv:1911.09070v1 ‚Äî figure 2 . Conventional FPN (Feature Pyramid Network) is limited by the one-way information flow. PANet added an extra bottom-up path for information flow. PANet achieved better accuracy but with the cost and more parameters and computations. The paper proposed several optimizations for cross-scale connections: . Remove Nodes that only have one input edge. If a node has only one input edge with no feature fusion, then it will have less contribution to the feature network that aims at fusing different features. . | Add an extra edge from the original input to output node if they are at the same level, in order to fuse more features without adding much cost. . | Treat each bidirectional (top-down &amp; bottom-up) path as one feature network layer, and repeat the same layer multiple times to enable more high-level feature fusion. . | Weighted Feature Fusion . While multi-scale fusion, input features are not simply summed up. The authors proposed to add additional weight for each input during feature fusion and let the network to learn the importance of each input feature. Out of three weighted fusion approaches ‚Äî Unbounded fusion: . Source: https://arxiv.org/abs/1911.09070 . Where W is a learnable weight that can be a scalar (per-feature), a vector (per-channel), or a multi-dimensional tensor (per-pixel). Since the scalar weight is unbounded, it could potentially cause training instability. So, Softmax-based fusion was tried for normalized weights. Softmax-based fusion: . . As softmax normalizes the weights to be the probability of range 0 to 1 which can denote the importance of each input. The softmax leads to a slowdown on GPU. Fast normalized fusion: . . –Ñ is added for numeric stability. It is 30% faster on GPU and gave almost as accurate results as softmax. . Final BiFPN integrates both the bidirectional cross-scale connections and the fast normalized fusion. . EfficientDet Architecture . Source: arXiv:1911.09070v1 ‚Äî figure 3 . EfficientDet follows one-stage-detection paradigm. A pre-trained EfficientNet backbone is used with BiFPN as the feature extractor. BiFPNN takes {P3, P4, P5, P6, P7} features from the EfficientNet backbone network and repeatedly applies bidirectional feature fusion. The fused features are fed to a class and bounding box network for predicting object class and bounding box. . References . EfficientDet: Scalable and Efficient Object Detection . EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks . Path Aggregation Network for Instance Segmentation . Deep Residual Learning for Image Recognition . . . Hope you liked the article. . üëâ Twitter: https://twitter.com/aniketmaurya üëâ Mail: aniketmaurya@outlook.com .",
            "url": "https://aniketmaurya.com/object%20detection/2020/01/13/EfficientDet.html",
            "relUrl": "/object%20detection/2020/01/13/EfficientDet.html",
            "date": " ‚Ä¢ Jan 13, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Face Recognition",
            "content": "AI is revolutionizing the world. Face recognition is one such spectrum of it. Almost everyone uses face recognition systems ‚Äî on our mobile, Facebook, Photo gallery apps or advanced security cameras. Learn how these systems are able to recognize our faces. . This article is inspired by the deeplearning.ai course on FaceNet. . . Face Verification vs. Face Recognition . Face Verification checks ‚Äúis this the claimed person?‚Äù. For example, in school, you go with your ID card and the invigilator verifies your face with the ID card. This is Face Verification. A mobile phone that unlocks using our face is also using face verification. It is 1:1 matching problem. . Now suppose the invigilator knows everyone by their name. So, you decide to go there without an ID card. The invigilator identifies your face and lets you in. This is Face Recognition. Face Recognition deals with ‚Äúwho is this person?‚Äù problem. We can say that it is a 1:K problem. . Why pixel-by-pixel comparison of images is¬†a¬†bad¬†idea? . A simple way for face verification can be comparing two images pixel-by-pixel and if the threshold between images is less than a threshold then we can say that they‚Äôre the same person. But since the pixel values in an image change dramatically even with a slight change of light, position or orientation. So, this method doesn‚Äôt work well. . Face Embedding come to the rescue . The embedding is represented by f (x), a d-dimensional vector. It encodes an image ‚Äòx‚Äô into a d-dimensional Euclidean space. The face embedding of two images of the same person are similar to each other and that of different persons are very different. . In ConvNet architectures, the initial layers learn to recognize basic patterns like straight lines, edges, circles, etc. and the deeper layers learn to recognize more complex patterns like numbers, faces, cars, etc. . To get our embedding we feed the image into a pre-trained model then run a forward propagation and extract the vector from some deeper Fully-Connected layer. You can learn the basics of CNN here. . How to finetune embeddings? . The Triplet Loss function takes the responsibility to push the embeddings of two images of the same person (Anchor and Positive) closer together while pulling the embeddings of two images of different persons (Anchor, Negative) further apart. . Source: Coursera . $||f(X1) ‚Äî f(X2)||¬≤$ is the degree of similarity between image X1 and X2. If it‚Äôs smaller than a chosen threshold then both are the same person. . If you‚Äôre wondering ‚Äúwhat this $|A|$ weird symbol is?‚Äù, it‚Äôs called Frobenius Norm. . The distance between Anchor and Positive images should be less and the distance between Anchor and Negative images should be high. . i.e. $‚à•f(Anchor) ‚Äî f(Positive)‚à•¬≤ ‚â§ ‚à•f(Anchor) ‚Äî f(Negative)‚à•¬≤$. . Training a Face recognition model is computationally expensive so it‚Äôs recommended to download a pre-trained model. . Start with creating a database of persons containing an embedding vector for each. . #create a dictionary database db = dict() #encoding(image_path) converts image to embedding db[&#39;person1&#39;] = encoding(&#39;person1.jpg&#39;) db[&#39;person2&#39;] = encoding(&#39;person2.jpg&#39;) db[&#39;person3&#39;] = encoding(&#39;person3.jpg&#39;) db[&#39;person4&#39;] = encoding(&#39;person4.jpg&#39;) db[&#39;person5&#39;] = encoding(&#39;person5.jpg&#39;) . Face Verification . Now that we have created our database, we can define a function that accepts image embedding and name of the person as the argument and it will verify if they are the same person. . def verify(embedding, person_name): # numpy.linalg.norm calculates the Frobenius Norm dist = np.linalg.norm(embedding - db[person_name]) # Chosen threshold is 0.7 if dist &lt; 0.7: print(&quot;Verified! Welcome &quot; + person_name) else: print(&quot;Person name and face didn&#39;t match!&quot;) . Hurray üòé! We created our Face Verification system. Now let‚Äôs create the Face Recognition System. If you remember, a person doesn‚Äôt need any ID in the face recognition system. He just needs to show his face to the camera. . Face Recognition . In Face recognition, the distance will be calculated for all the images in the database against the input embedding and the smallest distance has to be searched. . def recognize_me(input_embedding): # Set min_dist to infinity min_dist = np.inf # Iterate over the database to calulate distance for each person* for (name, emb) in db.items(): # Compute the distance* curr_dist = np.linalg.norm(input_embedding - emb) # identity is set to the name of the person from the database whose distance is smallest against the input encoding if curr_dist &lt; min_dist: min_dist = curr_dist identity = name if min_dist &gt; 0.7: print(&quot;Sorry! You‚Äôre not in the database.&quot;) else: print (&quot;Hi! Welcome &quot; + identity) . Congratulations!! üëèüëè You have created your own Face Recognition system. . You can provide your feedback in comment section below. . Follow me on Twitter .",
            "url": "https://aniketmaurya.com/tensorflow/face%20recognition/2019/01/07/face-recognition.html",
            "relUrl": "/tensorflow/face%20recognition/2019/01/07/face-recognition.html",
            "date": " ‚Ä¢ Jan 7, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". üëã I‚Äôm a Developer Advocate at Grid.ai | PyTorch Lightning ü•ë‚ö°Ô∏è. I build Intelligent Software with ML and create/maintain some cool open-source ML/DL/Python libraries. I‚Äôve created Gradsflow and Chitra - Python Libraries with aim to simplify AutoML and MLOps. . I love building problem solving products that leverage Artificial Intelligence. I‚Äôve expertise in building and deploying scalable Deep Learning system in production. I‚Äôve worked on complete end-to-end process of Deep Learning i.e. Data creation, Model Training, API Development to Deployment. . I believe in continuous learning, open-source and knowledge sharing. Started my YouTube channel to share educational video tutorials and tech Meetup on Machine Learning and Python Software development. . | | | | . Contact me to talk about ML/AI, freelancing or consulation. . This website is powered by fastpages .",
          "url": "https://aniketmaurya.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://aniketmaurya.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}